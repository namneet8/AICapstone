{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approach 3: Progressive Unfreezing - Stage 1\n",
    "==============================================\n",
    "Initial training with frozen YAMNet base model.\n",
    "Train only the new classification head.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Configuration\n",
    "FEATURES_DIR = '../data/features'\n",
    "MODELS_DIR = '../models/progressive_unfreezing'\n",
    "RESULTS_DIR = '../results/progressive_unfreezing'\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'\n",
    "\n",
    "TARGET_SR = 16000\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_STAGE1 = 30\n",
    "LEARNING_RATE_STAGE1 = 1e-3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(os.path.join(MODELS_DIR, 'stage1'), exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"APPROACH 3: PROGRESSIVE UNFREEZING - STAGE 1\")\n",
    "print(\"Training frozen base with new head\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Load Data\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Loading data...\")\n",
    "\n",
    "label_mapping = np.load(os.path.join(FEATURES_DIR, 'label_mapping.npy'),\n",
    "                       allow_pickle=True).item()\n",
    "categories = label_mapping['categories']\n",
    "num_classes = len(categories)\n",
    "\n",
    "train_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'train_metadata.csv'))\n",
    "val_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'val_metadata.csv'))\n",
    "test_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'test_metadata.csv'))\n",
    "\n",
    "print(f\"✓ Classes: {categories}\")\n",
    "print(f\"  Training:   {len(train_meta)} frames\")\n",
    "print(f\"  Validation: {len(val_meta)} frames\")\n",
    "print(f\"  Test:       {len(test_meta)} frames\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Load and Prepare Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n[2] Loading audio frames into memory...\")\n",
    "\n",
    "def load_all_frames(metadata_df, label='train'):\n",
    "    \"\"\"Load all frames into memory.\"\"\"\n",
    "    print(f\"  Loading {label} frames...\")\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        try:\n",
    "            audio = np.load(row['frame_path']).astype(np.float32)\n",
    "            audio_data.append(audio)\n",
    "            labels.append(int(row['label']))\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not load {row['frame_path']}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(audio_data), np.array(labels)\n",
    "\n",
    "train_X, train_y = load_all_frames(train_meta, 'training')\n",
    "val_X, val_y = load_all_frames(val_meta, 'validation')\n",
    "test_X, test_y = load_all_frames(test_meta, 'test')\n",
    "\n",
    "def create_dataset(X, y, batch_size, shuffle=True, seed=RANDOM_SEED):\n",
    "    \"\"\"Create tf.data.Dataset with proper shape.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X), seed=seed)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_X, train_y, BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(val_X, val_y, BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_dataset(test_X, test_y, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✓ Datasets created\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Build Model with Frozen Base\n",
    "# ============================================================================\n",
    "print(\"\\n[3] Building model with frozen YAMNet base...\")\n",
    "\n",
    "class ProgressiveUnfreezeModel(keras.Model):\n",
    "    \"\"\"YAMNet with progressive unfreezing capability.\"\"\"\n",
    "    def __init__(self, num_classes, yamnet_model_handle):\n",
    "        super(ProgressiveUnfreezeModel, self).__init__()\n",
    "        \n",
    "        self.yamnet = hub.KerasLayer(\n",
    "            yamnet_model_handle,\n",
    "            trainable=False,  # FROZEN in Stage 1\n",
    "            name='yamnet'\n",
    "        )\n",
    "        \n",
    "        self.classifier = keras.Sequential([\n",
    "            layers.Dense(256, activation='relu', name='fc1'),\n",
    "            layers.BatchNormalization(name='bn1'),\n",
    "            layers.Dropout(0.3, name='dropout1'),\n",
    "            layers.Dense(128, activation='relu', name='fc2'),\n",
    "            layers.BatchNormalization(name='bn2'),\n",
    "            layers.Dropout(0.2, name='dropout2'),\n",
    "            layers.Dense(num_classes, activation='softmax', name='output')\n",
    "        ], name='classifier')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass through YAMNet and classifier.\"\"\"\n",
    "        def process_waveform(waveform):\n",
    "            scores, embeddings, spectrogram = self.yamnet(waveform)\n",
    "            embedding = tf.reduce_mean(embeddings, axis=0)\n",
    "            return embedding\n",
    "        \n",
    "        embeddings = tf.map_fn(\n",
    "            process_waveform,\n",
    "            inputs,\n",
    "            fn_output_signature=tf.float32\n",
    "        )\n",
    "        \n",
    "        outputs = self.classifier(embeddings, training=training)\n",
    "        return outputs\n",
    "\n",
    "model = ProgressiveUnfreezeModel(num_classes, YAMNET_MODEL_HANDLE)\n",
    "\n",
    "# Build model\n",
    "dummy_input = tf.random.normal([1, int(TARGET_SR * 0.96)])\n",
    "_ = model(dummy_input)\n",
    "\n",
    "print(f\"✓ Model built\")\n",
    "print(f\"  Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Count frozen vs trainable\n",
    "frozen_count = sum([tf.size(w).numpy() for w in model.yamnet.trainable_weights])\n",
    "trainable_count = sum([tf.size(w).numpy() for w in model.classifier.trainable_weights])\n",
    "print(f\"  Frozen parameters (YAMNet): {frozen_count:,}\")\n",
    "print(f\"  Trainable parameters (Head): {trainable_count:,}\")\n",
    "\n",
    "print(f\"\\n  YAMNet trainable: {model.yamnet.trainable}\")\n",
    "print(f\"  Classifier trainable: {model.classifier.trainable}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Compile and Train - Stage 1\n",
    "# ============================================================================\n",
    "print(\"\\n[4] Compiling model (Stage 1)...\")\n",
    "\n",
    "# Class weights\n",
    "class_counts = pd.Series(train_y).value_counts().sort_index()\n",
    "total_samples = len(train_y)\n",
    "class_weights = {i: total_samples / (num_classes * count) \n",
    "                for i, count in enumerate(class_counts)}\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"✓ Model compiled\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE_STAGE1}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = os.path.join(MODELS_DIR, 'stage1', 'best_model_stage1.keras')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(\n",
    "    os.path.join(RESULTS_DIR, 'stage1_training_log.csv')\n",
    ")\n",
    "\n",
    "print(\"\\n[5] Training Stage 1 (frozen base, training head only)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_scheduler, csv_logger],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Stage 1 training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Plot Training History - Stage 1\n",
    "# ============================================================================\n",
    "print(\"\\n[6] Plotting training history...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_stage1.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_stage1.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Stage 1: Training Loss (Frozen Base)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage1.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_stage1.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Stage 1: Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'stage1_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Evaluate Stage 1\n",
    "# ============================================================================\n",
    "print(\"\\n[7] Evaluating Stage 1 on test set...\")\n",
    "\n",
    "model = keras.models.load_model(checkpoint_path, \n",
    "                               custom_objects={'ProgressiveUnfreezeModel': ProgressiveUnfreezeModel})\n",
    "\n",
    "y_pred = model.predict(test_X, verbose=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "test_accuracy = np.mean(test_y == y_pred)\n",
    "print(f\"\\n✓ Stage 1 Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(test_y, y_pred, target_names=categories, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "report_dict = classification_report(test_y, y_pred, target_names=categories, \n",
    "                                   output_dict=True)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=categories, yticklabels=categories,\n",
    "           cbar_kws={'label': 'Count'})\n",
    "plt.title('Stage 1: Confusion Matrix (Frozen Base)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'stage1_confusion_matrix.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Save Stage 1 Results\n",
    "# ============================================================================\n",
    "print(\"\\n[8] Saving Stage 1 artifacts...\")\n",
    "\n",
    "stage1_results = {\n",
    "    'stage': 1,\n",
    "    'description': 'Frozen YAMNet base, training head only',\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(report_dict['weighted avg']['precision']),\n",
    "    'test_recall': float(report_dict['weighted avg']['recall']),\n",
    "    'test_f1': float(report_dict['weighted avg']['f1-score']),\n",
    "    'epochs_trained': len(history_stage1.history['loss']),\n",
    "    'learning_rate': LEARNING_RATE_STAGE1,\n",
    "    'trainable_layers': 'Classifier head only',\n",
    "    'frozen_layers': 'YAMNet (all)'\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'stage1_results.json'), 'w') as f:\n",
    "    json.dump(stage1_results, f, indent=2)\n",
    "\n",
    "# Save best validation metrics\n",
    "val_metrics_stage1 = {\n",
    "    'best_val_loss': float(min(history_stage1.history['val_loss'])),\n",
    "    'best_val_accuracy': float(max(history_stage1.history['val_accuracy'])),\n",
    "    'final_val_loss': float(history_stage1.history['val_loss'][-1]),\n",
    "    'final_val_accuracy': float(history_stage1.history['val_accuracy'][-1])\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'stage1_val_metrics.json'), 'w') as f:\n",
    "    json.dump(val_metrics_stage1, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to {RESULTS_DIR}/\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 1 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Stage 1 Complete: Frozen Base Training\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Test Precision: {report_dict['weighted avg']['precision']:.4f}\")\n",
    "print(f\"  Test Recall:    {report_dict['weighted avg']['recall']:.4f}\")\n",
    "print(f\"  Test F1-Score:  {report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {len(history_stage1.history['loss'])}\")\n",
    "print(f\"  Best Val Accuracy: {max(history_stage1.history['val_accuracy']):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {history_stage1.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Frozen: YAMNet base\")\n",
    "print(f\"  Trainable: Classifier head only\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE_STAGE1}\")\n",
    "\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"  Model: {checkpoint_path}\")\n",
    "print(f\"  Results: {RESULTS_DIR}/stage1_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Next: Stage 2 - Unfreeze top layers of YAMNet\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
