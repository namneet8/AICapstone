{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dd4d57",
   "metadata": {},
   "source": [
    "Initial training with frozen YAMNet base model.\n",
    "Train only the new classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Configuration\n",
    "FEATURES_DIR = '../data/approach1/features'\n",
    "MODELS_DIR = '../models/models_approach3/progressive_unfreezing'\n",
    "RESULTS_DIR = '../results/results_progressive_unfreezing'\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'\n",
    "\n",
    "TARGET_SR = 16000\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_STAGE1 = 30\n",
    "LEARNING_RATE_STAGE1 = 1e-3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(os.path.join(MODELS_DIR, 'stage1'), exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = np.load(os.path.join(FEATURES_DIR, 'label_mapping.npy'),\n",
    "                       allow_pickle=True).item()\n",
    "categories = label_mapping['categories']\n",
    "num_classes = len(categories)\n",
    "\n",
    "train_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'train_metadata.csv'))\n",
    "val_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'val_metadata.csv'))\n",
    "test_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'test_metadata.csv'))\n",
    "\n",
    "print(f\"Classes: {categories}\")\n",
    "print(f\"  Training:   {len(train_meta)} frames\")\n",
    "print(f\"  Validation: {len(val_meta)} frames\")\n",
    "print(f\"  Test:       {len(test_meta)} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and Prepare Datasets\n",
    "\n",
    "print(\"\\nLoading audio frames into memory...\")\n",
    "\n",
    "def load_all_frames(metadata_df, label='train'):\n",
    "    \"\"\"Load all frames into memory.\"\"\"\n",
    "    print(f\"  Loading {label} frames...\")\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        try:\n",
    "            audio = np.load(row['frame_path']).astype(np.float32)\n",
    "            audio_data.append(audio)\n",
    "            labels.append(int(row['label']))\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not load {row['frame_path']}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(audio_data), np.array(labels)\n",
    "\n",
    "train_X, train_y = load_all_frames(train_meta, 'training')\n",
    "val_X, val_y = load_all_frames(val_meta, 'validation')\n",
    "test_X, test_y = load_all_frames(test_meta, 'test')\n",
    "\n",
    "def create_dataset(X, y, batch_size, shuffle=True, seed=RANDOM_SEED):\n",
    "    \"\"\"Create tf.data.Dataset with proper shape.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X), seed=seed)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_X, train_y, BATCH_SIZE, shuffle=True)\n",
    "val_dataset = create_dataset(val_X, val_y, BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_dataset(test_X, test_y, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Datasets created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build Model with Frozen Base\n",
    "print(\"\\nBuilding model with frozen YAMNet base...\")\n",
    "\n",
    "class ProgressiveUnfreezeModel(keras.Model):\n",
    "    \"\"\"YAMNet with progressive unfreezing capability.\"\"\"\n",
    "    def __init__(self, num_classes, yamnet_model_handle):\n",
    "        super(ProgressiveUnfreezeModel, self).__init__()\n",
    "        \n",
    "        self.yamnet = hub.KerasLayer(\n",
    "            yamnet_model_handle,\n",
    "            trainable=False,  # FROZEN in Stage 1\n",
    "            name='yamnet'\n",
    "        )\n",
    "        \n",
    "        self.classifier = keras.Sequential([\n",
    "            layers.Dense(256, activation='relu', name='fc1'),\n",
    "            layers.BatchNormalization(name='bn1'),\n",
    "            layers.Dropout(0.3, name='dropout1'),\n",
    "            layers.Dense(128, activation='relu', name='fc2'),\n",
    "            layers.BatchNormalization(name='bn2'),\n",
    "            layers.Dropout(0.2, name='dropout2'),\n",
    "            layers.Dense(num_classes, activation='softmax', name='output')\n",
    "        ], name='classifier')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass through YAMNet and classifier.\"\"\"\n",
    "        def process_waveform(waveform):\n",
    "            scores, embeddings, spectrogram = self.yamnet(waveform)\n",
    "            embedding = tf.reduce_mean(embeddings, axis=0)\n",
    "            return embedding\n",
    "        \n",
    "        embeddings = tf.map_fn(\n",
    "            process_waveform,\n",
    "            inputs,\n",
    "            fn_output_signature=tf.float32\n",
    "        )\n",
    "        \n",
    "        outputs = self.classifier(embeddings, training=training)\n",
    "        return outputs\n",
    "\n",
    "model = ProgressiveUnfreezeModel(num_classes, YAMNET_MODEL_HANDLE)\n",
    "\n",
    "# Build model\n",
    "dummy_input = tf.random.normal([1, int(TARGET_SR * 0.96)])\n",
    "_ = model(dummy_input)\n",
    "\n",
    "print(f\"Model built\")\n",
    "print(f\"  Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Count frozen vs trainable\n",
    "frozen_count = sum([tf.size(w).numpy() for w in model.yamnet.trainable_weights])\n",
    "trainable_count = sum([tf.size(w).numpy() for w in model.classifier.trainable_weights])\n",
    "print(f\"  Frozen parameters (YAMNet): {frozen_count:,}\")\n",
    "print(f\"  Trainable parameters (Head): {trainable_count:,}\")\n",
    "\n",
    "print(f\"\\n  YAMNet trainable: {model.yamnet.trainable}\")\n",
    "print(f\"  Classifier trainable: {model.classifier.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile and Train - Stage 1\n",
    "print(\"\\nCompiling model (Stage 1)...\")\n",
    "\n",
    "# Class weights\n",
    "class_counts = pd.Series(train_y).value_counts().sort_index()\n",
    "total_samples = len(train_y)\n",
    "class_weights = {i: total_samples / (num_classes * count) \n",
    "                for i, count in enumerate(class_counts)}\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model compiled\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE_STAGE1}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = os.path.join(MODELS_DIR, 'stage1', 'best_model_stage1.keras')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(\n",
    "    os.path.join(RESULTS_DIR, 'stage1_training_log.csv')\n",
    ")\n",
    "\n",
    "print(\"\\n[5] Training Stage 1 (frozen base, training head only)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_scheduler, csv_logger],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStage 1 training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe660cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot Training History - Stage 1\n",
    "print(\"\\nPlotting training history...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_stage1.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_stage1.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Stage 1: Training Loss (Frozen Base)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage1.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_stage1.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Stage 1: Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'stage1_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training history saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluate Stage 1\n",
    "print(\"\\n[7] Evaluating Stage 1 on test set...\")\n",
    "\n",
    "model = keras.models.load_model(checkpoint_path, \n",
    "                               custom_objects={'ProgressiveUnfreezeModel': ProgressiveUnfreezeModel})\n",
    "\n",
    "y_pred = model.predict(test_X, verbose=0)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "test_accuracy = np.mean(test_y == y_pred)\n",
    "print(f\"\\nStage 1 Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "report = classification_report(test_y, y_pred, target_names=categories, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "report_dict = classification_report(test_y, y_pred, target_names=categories, \n",
    "                                   output_dict=True)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=categories, yticklabels=categories,\n",
    "           cbar_kws={'label': 'Count'})\n",
    "plt.title('Stage 1: Confusion Matrix (Frozen Base)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, 'stage1_confusion_matrix.png'), dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de863d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Stage 1 Results\n",
    "print(\"\\nSaving Stage 1 artifacts...\")\n",
    "\n",
    "stage1_results = {\n",
    "    'stage': 1,\n",
    "    'description': 'Frozen YAMNet base, training head only',\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(report_dict['weighted avg']['precision']),\n",
    "    'test_recall': float(report_dict['weighted avg']['recall']),\n",
    "    'test_f1': float(report_dict['weighted avg']['f1-score']),\n",
    "    'epochs_trained': len(history_stage1.history['loss']),\n",
    "    'learning_rate': LEARNING_RATE_STAGE1,\n",
    "    'trainable_layers': 'Classifier head only',\n",
    "    'frozen_layers': 'YAMNet (all)'\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'stage1_results.json'), 'w') as f:\n",
    "    json.dump(stage1_results, f, indent=2)\n",
    "\n",
    "# Save best validation metrics\n",
    "val_metrics_stage1 = {\n",
    "    'best_val_loss': float(min(history_stage1.history['val_loss'])),\n",
    "    'best_val_accuracy': float(max(history_stage1.history['val_accuracy'])),\n",
    "    'final_val_loss': float(history_stage1.history['val_loss'][-1]),\n",
    "    'final_val_accuracy': float(history_stage1.history['val_accuracy'][-1])\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, 'stage1_val_metrics.json'), 'w') as f:\n",
    "    json.dump(val_metrics_stage1, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {RESULTS_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Summary\n",
    "\n",
    "print(f\"\\nStage 1 Complete: Frozen Base Training\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Test Precision: {report_dict['weighted avg']['precision']:.4f}\")\n",
    "print(f\"  Test Recall:    {report_dict['weighted avg']['recall']:.4f}\")\n",
    "print(f\"  Test F1-Score:  {report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {len(history_stage1.history['loss'])}\")\n",
    "print(f\"  Best Val Accuracy: {max(history_stage1.history['val_accuracy']):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {history_stage1.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Frozen: YAMNet base\")\n",
    "print(f\"  Trainable: Classifier head only\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE_STAGE1}\")\n",
    "\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"  Model: {checkpoint_path}\")\n",
    "print(f\"  Results: {RESULTS_DIR}/stage1_results.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
