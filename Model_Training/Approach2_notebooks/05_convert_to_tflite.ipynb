{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d28885",
   "metadata": {},
   "source": [
    "Converts the fine-tuned Keras model to TFLite format with various optimization strategies for mobile deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe816449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "MODELS_DIR = '../models/models_approach2/yamnet_finetuned'\n",
    "TFLITE_DIR = '../models/models_approach2/tflite'\n",
    "FEATURES_DIR = '../data/approach2/features'\n",
    "TEST_SAMPLES = 100  # Number of samples for testing\n",
    "\n",
    "os.makedirs(TFLITE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddea16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Fine-Tuned Model\n",
    "print(\"\\nLoading fine-tuned model...\")\n",
    "\n",
    "model_path = os.path.join(MODELS_DIR, 'yamnet_finetuned_final.keras')\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model not found at {model_path}\")\n",
    "    print(\"Trying best_model.keras...\")\n",
    "    model_path = os.path.join(MODELS_DIR, 'best_model.keras')\n",
    "\n",
    "model = keras.models.load_model(model_path)\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "# Load model config\n",
    "config_path = os.path.join(MODELS_DIR, 'model_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "categories = model_config['categories']\n",
    "num_classes = model_config['num_classes']\n",
    "TARGET_SR = model_config['sample_rate']\n",
    "\n",
    "print(f\"  Classes: {categories}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare Representative Dataset for Quantization\n",
    "print(\"\\nPreparing representative dataset for quantization...\")\n",
    "\n",
    "# Load test data for representative samples\n",
    "test_meta = pd.read_csv(os.path.join(FEATURES_DIR, 'test_metadata.csv'))\n",
    "sample_paths = test_meta['frame_path'].sample(min(TEST_SAMPLES, len(test_meta)), \n",
    "                                               random_state=42).tolist()\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"Generator for representative dataset.\"\"\"\n",
    "    for path in sample_paths:\n",
    "        try:\n",
    "            audio = np.load(path)\n",
    "            # Ensure correct shape and dtype\n",
    "            audio = audio.astype(np.float32)\n",
    "            audio = np.expand_dims(audio, axis=0)  # Add batch dimension\n",
    "            yield [audio]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Prepared {len(sample_paths)} representative samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Convert to TFLite - Float32 (No Quantization)\n",
    "print(\"\\nConverting to TFLite (Float32 - No Quantization)...\")\n",
    "\n",
    "converter_float32 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_float32 = converter_float32.convert()\n",
    "\n",
    "# Save\n",
    "tflite_float32_path = os.path.join(TFLITE_DIR, 'yamnet_finetuned_float32.tflite')\n",
    "with open(tflite_float32_path, 'wb') as f:\n",
    "    f.write(tflite_float32)\n",
    "\n",
    "float32_size = len(tflite_float32) / (1024 * 1024)\n",
    "print(f\"Float32 model saved to {tflite_float32_path}\")\n",
    "print(f\"  Size: {float32_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99845ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert to TFLite - Dynamic Range Quantization\n",
    "print(\"\\nConverting to TFLite (Dynamic Range Quantization)...\")\n",
    "\n",
    "converter_dynamic = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_dynamic.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_dynamic = converter_dynamic.convert()\n",
    "\n",
    "# Save\n",
    "tflite_dynamic_path = os.path.join(TFLITE_DIR, 'yamnet_finetuned_dynamic.tflite')\n",
    "with open(tflite_dynamic_path, 'wb') as f:\n",
    "    f.write(tflite_dynamic)\n",
    "\n",
    "dynamic_size = len(tflite_dynamic) / (1024 * 1024)\n",
    "compression_ratio = float32_size / dynamic_size\n",
    "print(f\"Dynamic quantized model saved to {tflite_dynamic_path}\")\n",
    "print(f\"  Size: {dynamic_size:.2f} MB\")\n",
    "print(f\"  Compression: {compression_ratio:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906555c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert to TFLite - Float16 Quantization\n",
    "print(\"\\nConverting to TFLite (Float16 Quantization)...\")\n",
    "\n",
    "converter_float16 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_float16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_float16.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_float16 = converter_float16.convert()\n",
    "\n",
    "# Save\n",
    "tflite_float16_path = os.path.join(TFLITE_DIR, 'yamnet_finetuned_float16.tflite')\n",
    "with open(tflite_float16_path, 'wb') as f:\n",
    "    f.write(tflite_float16)\n",
    "\n",
    "float16_size = len(tflite_float16) / (1024 * 1024)\n",
    "compression_ratio_16 = float32_size / float16_size\n",
    "print(f\"Float16 quantized model saved to {tflite_float16_path}\")\n",
    "print(f\"  Size: {float16_size:.2f} MB\")\n",
    "print(f\"  Compression: {compression_ratio_16:.2f}x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36698d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Convert to TFLite - Full Integer Quantization (INT8)\n",
    "\n",
    "print(\"\\nConverting to TFLite (Full Integer Quantization - INT8)...\")\n",
    "\n",
    "converter_int8 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_int8.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# For full integer quantization\n",
    "converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_int8.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter_int8.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "try:\n",
    "    tflite_int8 = converter_int8.convert()\n",
    "    \n",
    "    # Save\n",
    "    tflite_int8_path = os.path.join(TFLITE_DIR, 'yamnet_finetuned_int8.tflite')\n",
    "    with open(tflite_int8_path, 'wb') as f:\n",
    "        f.write(tflite_int8)\n",
    "    \n",
    "    int8_size = len(tflite_int8) / (1024 * 1024)\n",
    "    compression_ratio_int8 = float32_size / int8_size\n",
    "    print(f\"INT8 quantized model saved to {tflite_int8_path}\")\n",
    "    print(f\"  Size: {int8_size:.2f} MB\")\n",
    "    print(f\"  Compression: {compression_ratio_int8:.2f}x\")\n",
    "    \n",
    "    int8_available = True\n",
    "except Exception as e:\n",
    "    print(f\"INT8 quantization failed: {str(e)}\")\n",
    "    print(\"  This is common with complex models. Using Float16 as most compressed version.\")\n",
    "    int8_available = False\n",
    "    tflite_int8_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Test TFLite Models - Accuracy and Latency\n",
    "print(\"\\nTesting TFLite models (accuracy and latency)...\")\n",
    "\n",
    "def test_tflite_model(tflite_path, test_samples=50):\n",
    "    \"\"\"Test a TFLite model.\"\"\"\n",
    "    # Load interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(f\"\\n  Input details: {input_details[0]['shape']}, {input_details[0]['dtype']}\")\n",
    "    print(f\"  Output details: {output_details[0]['shape']}, {output_details[0]['dtype']}\")\n",
    "    \n",
    "    # Test on samples\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    latencies = []\n",
    "    \n",
    "    test_paths = test_meta.sample(min(test_samples, len(test_meta)), \n",
    "                                  random_state=42)\n",
    "    \n",
    "    for _, row in test_paths.iterrows():\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio = np.load(row['frame_path']).astype(np.float32)\n",
    "            audio = np.expand_dims(audio, axis=0)\n",
    "            \n",
    "            # Convert input if needed\n",
    "            if input_details[0]['dtype'] == np.int8:\n",
    "                # Quantize input\n",
    "                input_scale, input_zero_point = input_details[0]['quantization']\n",
    "                audio = audio / input_scale + input_zero_point\n",
    "                audio = audio.astype(np.int8)\n",
    "            \n",
    "            # Set input\n",
    "            interpreter.set_tensor(input_details[0]['index'], audio)\n",
    "            \n",
    "            # Run inference\n",
    "            start_time = time.time()\n",
    "            interpreter.invoke()\n",
    "            latency = (time.time() - start_time) * 1000  # ms\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            # Get output\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            \n",
    "            # Dequantize output if needed\n",
    "            if output_details[0]['dtype'] == np.int8:\n",
    "                output_scale, output_zero_point = output_details[0]['quantization']\n",
    "                output = (output.astype(np.float32) - output_zero_point) * output_scale\n",
    "            \n",
    "            pred = np.argmax(output[0])\n",
    "            \n",
    "            if pred == row['label']:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error processing sample: {e}\")\n",
    "            continue\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_latency = np.mean(latencies) if latencies else 0\n",
    "    std_latency = np.std(latencies) if latencies else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_latency_ms': avg_latency,\n",
    "        'std_latency_ms': std_latency,\n",
    "        'samples_tested': total\n",
    "    }\n",
    "\n",
    "# Test all models\n",
    "models_to_test = [\n",
    "    ('Float32', tflite_float32_path, float32_size),\n",
    "    ('Dynamic Quant', tflite_dynamic_path, dynamic_size),\n",
    "    ('Float16', tflite_float16_path, float16_size),\n",
    "]\n",
    "\n",
    "if int8_available:\n",
    "    models_to_test.append(('INT8', tflite_int8_path, int8_size))\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_path, model_size in models_to_test:\n",
    "    print(f\"\\nTesting {model_name} model...\")\n",
    "    test_results = test_tflite_model(model_path, test_samples=TEST_SAMPLES)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Size (MB)': model_size,\n",
    "        'Accuracy': test_results['accuracy'],\n",
    "        'Avg Latency (ms)': test_results['avg_latency_ms'],\n",
    "        'Std Latency (ms)': test_results['std_latency_ms'],\n",
    "        'Samples': test_results['samples_tested']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"  Avg Latency: {test_results['avg_latency_ms']:.2f} ms\")\n",
    "    print(f\"  Std Latency: {test_results['std_latency_ms']:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e80129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Compare Models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TFLITE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "results_df.to_csv(os.path.join(TFLITE_DIR, 'tflite_comparison.csv'), index=False)\n",
    "\n",
    "# Visualize comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Size comparison\n",
    "axes[0].bar(results_df['Model'], results_df['Size (MB)'], color='steelblue')\n",
    "axes[0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[0].set_xlabel('Model Type', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Size (MB)']):\n",
    "    axes[0].text(i, v + 0.5, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].bar(results_df['Model'], results_df['Accuracy'], color='coral')\n",
    "axes[1].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_xlabel('Model Type', fontsize=12)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "# Latency comparison\n",
    "axes[2].bar(results_df['Model'], results_df['Avg Latency (ms)'], color='lightgreen')\n",
    "axes[2].set_title('Inference Latency Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Average Latency (ms)', fontsize=12)\n",
    "axes[2].set_xlabel('Model Type', fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Avg Latency (ms)']):\n",
    "    axes[2].text(i, v + 1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(TFLITE_DIR, 'tflite_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Comparison plot saved to {TFLITE_DIR}/tflite_comparison.png\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Compare Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TFLITE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "results_df.to_csv(os.path.join(TFLITE_DIR, 'tflite_comparison.csv'), index=False)\n",
    "\n",
    "# Visualize comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Size comparison\n",
    "axes[0].bar(results_df['Model'], results_df['Size (MB)'], color='steelblue')\n",
    "axes[0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[0].set_xlabel('Model Type', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Size (MB)']):\n",
    "    axes[0].text(i, v + 0.5, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].bar(results_df['Model'], results_df['Accuracy'], color='coral')\n",
    "axes[1].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_xlabel('Model Type', fontsize=12)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "# Latency comparison\n",
    "axes[2].bar(results_df['Model'], results_df['Avg Latency (ms)'], color='lightgreen')\n",
    "axes[2].set_title('Inference Latency Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Average Latency (ms)', fontsize=12)\n",
    "axes[2].set_xlabel('Model Type', fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(results_df['Avg Latency (ms)']):\n",
    "    axes[2].text(i, v + 1, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(TFLITE_DIR, 'tflite_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Comparison plot saved to {TFLITE_DIR}/tflite_comparison.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model based on accuracy-size tradeoff\n",
    "results_df['Score'] = results_df['Accuracy'] / (results_df['Size (MB)'] ** 0.5)\n",
    "best_idx = results_df['Score'].idxmax()\n",
    "best_model = results_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\nRecommended model: {best_model['Model']}\")\n",
    "print(f\"  - Size: {best_model['Size (MB)']:.2f} MB\")\n",
    "print(f\"  - Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"  - Avg Latency: {best_model['Avg Latency (ms)']:.2f} ms\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
