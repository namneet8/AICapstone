{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4e9b89",
   "metadata": {},
   "source": [
    "Combines frozen YAMNet + Scaler + Classifier → TensorFlow SavedModel\n",
    "\n",
    "Supports: LogisticRegression (TF ops), Others → Keras MLP (retrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a2c6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99e40dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../models/models_approach1'\n",
    "RESULTS_DIR = '../results/results_approach1'\n",
    "SAVED_MODELS_DIR = os.path.join(MODELS_DIR, 'saved_models')\n",
    "YAMNET_URL = 'https://tfhub.dev/google/yamnet/1'\n",
    "TARGET_SR = 16000\n",
    "\n",
    "os.makedirs(SAVED_MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10194d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet loaded\n"
     ]
    }
   ],
   "source": [
    "# LOAD COMPONENTS\n",
    "\n",
    "label_encoder = joblib.load(os.path.join(MODELS_DIR, 'label_encoder.pkl'))\n",
    "scaler = joblib.load(os.path.join(MODELS_DIR, 'feature_scaler.pkl'))\n",
    "comparison_df = pd.read_csv(os.path.join(RESULTS_DIR, 'model_comparison.csv'))\n",
    "\n",
    "classifiers = {}\n",
    "for name in comparison_df['Model']:\n",
    "    path = os.path.join(MODELS_DIR, f'{name.lower()}_model.pkl')\n",
    "    classifiers[name] = joblib.load(path)\n",
    "\n",
    "yamnet = hub.load(YAMNET_URL)\n",
    "print(\"YAMNet loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3af163a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RETRAINING NON-LR CLASSIFIERS AS KERAS MLP\n",
      "  XGBoost: Retraining as Keras MLP...\n",
      "    Retrained\n",
      "  MLP: Retraining as Keras MLP...\n",
      "    Retrained\n",
      "  Random_Forest: Retraining as Keras MLP...\n",
      "    Retrained\n",
      "  SVM: Retraining as Keras MLP...\n",
      "    Retrained\n",
      "  Logistic_Regression: Using original (TF-op compatible)\n"
     ]
    }
   ],
   "source": [
    "#RETRAIN NON-LINEAR CLASSIFIERS AS KERAS MLP\n",
    "\n",
    "print(\"\\nRETRAINING NON-LR CLASSIFIERS AS KERAS MLP\")\n",
    "\n",
    "X_train = np.load('../data/approach1/train_set/X_train.npy')  \n",
    "y_train = np.load('../data/approach1/train_set/y_train.npy')\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "keras_classifiers = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if isinstance(clf, LogisticRegression):\n",
    "        print(f\"  {name}: Using original (TF-op compatible)\")\n",
    "        keras_classifiers[name] = clf\n",
    "        continue\n",
    "\n",
    "    print(f\"  {name}: Retraining as Keras MLP...\")\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "\n",
    "    # Match original architecture\n",
    "    if 'MLP' in name:\n",
    "        layers = clf.hidden_layer_sizes\n",
    "    elif 'Random' in name or 'XGBoost' in name:\n",
    "        layers = (256, 128)\n",
    "    elif 'SVM' in name:\n",
    "        layers = (512,)\n",
    "    else:\n",
    "        layers = (256,)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    for i,h in enumerate(layers if isinstance(layers, (list, tuple)) else [layers]):\n",
    "        model.add(tf.keras.layers.Dense(h, activation='relu'))\n",
    "        if h > 64: model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    model.fit(X_train_scaled, y_train, epochs=50, batch_size=64, verbose=0,\n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                            monitor='loss',           # ← CHANGE THIS\n",
    "                            patience=10,\n",
    "                            restore_best_weights=True\n",
    "                        )])\n",
    "\n",
    "    keras_classifiers[name] = model\n",
    "    print(f\"    Retrained\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "531d3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD FULL KERAS MODEL\n",
    "\n",
    "class YamnetEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, yamnet_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.yamnet_model = yamnet_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, 15360)\n",
    "        def map_fn(wave):\n",
    "            scores, embeddings, _ = self.yamnet_model(wave)\n",
    "            return tf.reduce_mean(embeddings, axis=0)\n",
    "        return tf.map_fn(map_fn, inputs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def build_full_model(classifier, name):\n",
    "    inputs = tf.keras.Input(shape=(15360,), dtype=tf.float32, name=\"audio\")\n",
    "\n",
    "    yamnet_layer = YamnetEmbedding(yamnet)\n",
    "    pooled = yamnet_layer(inputs)\n",
    "\n",
    "    scaled = (pooled - scaler.mean_) / scaler.scale_\n",
    "\n",
    "    if isinstance(classifier, LogisticRegression):\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            len(label_encoder.classes_),\n",
    "            kernel_initializer=tf.constant_initializer(classifier.coef_.T),\n",
    "            bias_initializer=tf.constant_initializer(classifier.intercept_)\n",
    "        )(scaled)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "    else:\n",
    "        probs = classifier(scaled)\n",
    "\n",
    "    model = tf.keras.Model(inputs, probs)\n",
    "    model.compile()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "690aa27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: XGBoost\n",
      "  Probs sum: 1.000\n",
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\xgboost_full\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\xgboost_full\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: ../models/models_approach1\\saved_models\\xgboost_full\n",
      "\n",
      "Building: MLP\n",
      "  Probs sum: 1.000\n",
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\mlp_full\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\mlp_full\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: ../models/models_approach1\\saved_models\\mlp_full\n",
      "\n",
      "Building: Random_Forest\n",
      "  Probs sum: 1.000\n",
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\random_forest_full\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\random_forest_full\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: ../models/models_approach1\\saved_models\\random_forest_full\n",
      "\n",
      "Building: SVM\n",
      "  Probs sum: 1.000\n",
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\svm_full\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\svm_full\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: ../models/models_approach1\\saved_models\\svm_full\n",
      "\n",
      "Building: Logistic_Regression\n",
      "  Probs sum: 1.000\n",
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\logistic_regression_full\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/models_approach1\\saved_models\\logistic_regression_full\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: ../models/models_approach1\\saved_models\\logistic_regression_full\n"
     ]
    }
   ],
   "source": [
    "# BUILD & SAVE ALL\n",
    "\n",
    "saved_paths = {}\n",
    "for name, clf in keras_classifiers.items():\n",
    "    print(f\"\\nBuilding: {name}\")\n",
    "    full_model = build_full_model(clf, name)\n",
    "\n",
    "    # Test\n",
    "    test_audio = np.random.randn(15360).astype(np.float32)\n",
    "    pred = full_model.predict(test_audio[None, ...], verbose=0)[0]\n",
    "    print(f\"  Probs sum: {pred.sum():.3f}\")\n",
    "\n",
    "    path = os.path.join(SAVED_MODELS_DIR, f\"{name.lower()}_full\")\n",
    "    full_model.save(path, save_format='tf')\n",
    "    saved_paths[name] = path\n",
    "    print(f\"  Saved: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9c999e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERIFYING SAVEDMODELS\n",
      "  XGBoost: OK, sum=1.000\n",
      "  MLP: OK, sum=1.000\n",
      "  Random_Forest: OK, sum=1.000\n",
      "  SVM: OK, sum=1.000\n",
      "  Logistic_Regression: OK, sum=1.000\n",
      "\n",
      "INVENTORY:\n",
      "              Model  Val_F1                                                             Path Size_MB\n",
      "            XGBoost  0.9257             ../models/models_approach1\\saved_models\\xgboost_full     3.5\n",
      "                MLP  0.9250                 ../models/models_approach1\\saved_models\\mlp_full     3.5\n",
      "      Random_Forest  0.9213       ../models/models_approach1\\saved_models\\random_forest_full     3.5\n",
      "                SVM  0.9209                 ../models/models_approach1\\saved_models\\svm_full     3.5\n",
      "Logistic_Regression  0.9191 ../models/models_approach1\\saved_models\\logistic_regression_full     3.6\n"
     ]
    }
   ],
   "source": [
    "# VERIFY\n",
    "\n",
    "print(\"\\nVERIFYING SAVEDMODELS\")\n",
    "for name, path in saved_paths.items():\n",
    "    loaded = tf.keras.models.load_model(path, custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "    test_out = loaded.predict(np.random.randn(1, 15360).astype(np.float32), verbose=0)[0]\n",
    "    print(f\"  {name}: OK, sum={test_out.sum():.3f}\")\n",
    "\n",
    "\n",
    "inventory = []\n",
    "for _, row in comparison_df.iterrows():\n",
    "    name = row['Model']\n",
    "    size = sum(os.path.getsize(f) for f in tf.io.gfile.glob(f\"{saved_paths[name]}/*\")) / 1e6\n",
    "    inventory.append({\n",
    "        'Model': name,\n",
    "        'Val_F1': row['Val F1'],\n",
    "        'Path': saved_paths[name],\n",
    "        'Size_MB': f\"{size:.1f}\"\n",
    "    })\n",
    "\n",
    "inv_df = pd.DataFrame(inventory).sort_values('Val_F1', ascending=False)\n",
    "print(\"\\nINVENTORY:\")\n",
    "print(inv_df.to_string(index=False))\n",
    "inv_df.to_csv(os.path.join(RESULTS_DIR, 'full_model_inventory.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
