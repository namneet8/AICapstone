{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8045e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60c1732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_wav(mp3_path, wav_path, sample_rate=16000):\n",
    "    \"\"\"Convert MP3 to WAV at 16kHz (YAMNet requirement)\"\"\"\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    audio = audio.set_frame_rate(sample_rate)\n",
    "    audio = audio.set_channels(1)  # Mono\n",
    "    audio.export(wav_path, format='wav')\n",
    "    return wav_path\n",
    "\n",
    "def normalize_audio(audio_data):\n",
    "    \"\"\"Normalize audio to [-1, 1] range\"\"\"\n",
    "    max_val = np.abs(audio_data).max()\n",
    "    if max_val > 0:\n",
    "        return audio_data / max_val\n",
    "    return audio_data\n",
    "\n",
    "def trim_silence(audio_data, sr, top_db=20):\n",
    "    \"\"\"Trim silence from beginning and end\"\"\"\n",
    "    trimmed, _ = librosa.effects.trim(audio_data, top_db=top_db)\n",
    "    return trimmed\n",
    "\n",
    "def create_fixed_length_chunks(audio_data, sr, chunk_duration=5.0):\n",
    "    \"\"\"\n",
    "    Split audio into fixed-length chunks\n",
    "    YAMNet expects variable length, but we create chunks for consistency\n",
    "    \"\"\"\n",
    "    chunk_samples = int(chunk_duration * sr)\n",
    "    chunks = []\n",
    "    \n",
    "    # If audio is shorter than chunk_duration, pad it\n",
    "    if len(audio_data) < chunk_samples:\n",
    "        padded = np.pad(audio_data, (0, chunk_samples - len(audio_data)), mode='constant')\n",
    "        chunks.append(padded)\n",
    "    else:\n",
    "        # Split into overlapping chunks (50% overlap for data augmentation)\n",
    "        hop_length = chunk_samples // 2\n",
    "        for i in range(0, len(audio_data) - chunk_samples + 1, hop_length):\n",
    "            chunk = audio_data[i:i + chunk_samples]\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def add_noise(audio_data, noise_factor=0.005):\n",
    "    \"\"\"Add random noise for augmentation\"\"\"\n",
    "    noise = np.random.randn(len(audio_data))\n",
    "    augmented = audio_data + noise_factor * noise\n",
    "    return augmented\n",
    "\n",
    "def time_shift(audio_data, sr, shift_max=0.2):\n",
    "    \"\"\"Shift audio in time\"\"\"\n",
    "    shift = np.random.randint(int(sr * -shift_max), int(sr * shift_max))\n",
    "    return np.roll(audio_data, shift)\n",
    "\n",
    "def pitch_shift(audio_data, sr, n_steps=2):\n",
    "    \"\"\"Shift pitch up or down\"\"\"\n",
    "    return librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282077d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(input_path, output_dir, class_name, \n",
    "                       apply_augmentation=True, chunk_duration=5.0):\n",
    "    \"\"\"Process a single audio file\"\"\"\n",
    "    # Load audio at 16kHz\n",
    "    audio, sr = librosa.load(input_path, sr=16000)\n",
    "    \n",
    "    # Normalize\n",
    "    audio = normalize_audio(audio)\n",
    "    \n",
    "    # Trim silence\n",
    "    audio = trim_silence(audio, sr)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = create_fixed_length_chunks(audio, sr, chunk_duration)\n",
    "    \n",
    "    processed_files = []\n",
    "    base_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    \n",
    "    # Save original chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_chunk{i}.wav\")\n",
    "        sf.write(output_path, chunk, sr)\n",
    "        processed_files.append(output_path)\n",
    "    \n",
    "    # Apply augmentation if requested\n",
    "    if apply_augmentation and len(chunks) > 0:\n",
    "        # Use first chunk for augmentation\n",
    "        chunk = chunks[0]\n",
    "        \n",
    "        # Noise augmentation\n",
    "        noisy = add_noise(chunk)\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_noisy.wav\")\n",
    "        sf.write(output_path, noisy, sr)\n",
    "        processed_files.append(output_path)\n",
    "        \n",
    "        # Time shift augmentation\n",
    "        shifted = time_shift(chunk, sr)\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_shifted.wav\")\n",
    "        sf.write(output_path, shifted, sr)\n",
    "        processed_files.append(output_path)\n",
    "        \n",
    "        # Pitch shift augmentation\n",
    "        pitched = pitch_shift(chunk, sr, n_steps=np.random.choice([-2, 2]))\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}_pitched.wav\")\n",
    "        sf.write(output_path, pitched, sr)\n",
    "        processed_files.append(output_path)\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "def process_dataset(input_dir='../data/raw', output_dir='../data/processed', \n",
    "                   apply_augmentation=True):\n",
    "    \"\"\"Process entire dataset\"\"\"\n",
    "    dataset_info = {\n",
    "        'classes': [],\n",
    "        'files': {},\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_input_path = os.path.join(input_dir, class_name)\n",
    "        if not os.path.isdir(class_input_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing class: {class_name}\")\n",
    "        dataset_info['classes'].append(class_name)\n",
    "        \n",
    "        # Create output directory for this class\n",
    "        class_output_path = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "        \n",
    "        # Process all audio files in this class\n",
    "        audio_files = [f for f in os.listdir(class_input_path) if f.endswith('.mp3')]\n",
    "        dataset_info['files'][class_name] = []\n",
    "        \n",
    "        for audio_file in tqdm(audio_files, desc=f\"Processing {class_name}\"):\n",
    "            input_path = os.path.join(class_input_path, audio_file)\n",
    "            \n",
    "            try:\n",
    "                processed = process_audio_file(\n",
    "                    input_path, \n",
    "                    class_output_path, \n",
    "                    class_name,\n",
    "                    apply_augmentation=apply_augmentation\n",
    "                )\n",
    "                dataset_info['files'][class_name].extend(processed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_file}: {e}\")\n",
    "        \n",
    "        # Statistics\n",
    "        dataset_info['statistics'][class_name] = {\n",
    "            'original_files': len(audio_files),\n",
    "            'processed_files': len(dataset_info['files'][class_name])\n",
    "        }\n",
    "    \n",
    "    return dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfiguration:\")\n",
    "print(\"- Target sample rate: 16kHz\")\n",
    "print(\"- Audio normalization: Enabled\")\n",
    "print(\"- Silence trimming: Enabled\")\n",
    "print(\"- Chunk duration: 5 seconds\")\n",
    "print(\"- Augmentation: Enabled (noise, time shift, pitch shift)\")\n",
    "\n",
    "dataset_info = process_dataset(\n",
    "    input_dir='../data/raw',\n",
    "    output_dir='../data/processed',\n",
    "    apply_augmentation=True\n",
    ")\n",
    "\n",
    "# Save dataset info\n",
    "with open('../data/processed/dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "# =========================\n",
    "# 4. VISUALIZATION\n",
    "# =========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for class_name in dataset_info['classes']:\n",
    "    stats = dataset_info['statistics'][class_name]\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Original files: {stats['original_files']}\")\n",
    "    print(f\"  Processed files: {stats['processed_files']}\")\n",
    "    print(f\"  Augmentation factor: {stats['processed_files'] / stats['original_files']:.1f}x\")\n",
    "\n",
    "# Visualize before/after comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Select first class for visualization\n",
    "first_class = dataset_info['classes'][0]\n",
    "first_class_dir = os.path.join('../data/processed', first_class)\n",
    "sample_files = os.listdir(first_class_dir)[:4]  # Get 4 samples\n",
    "\n",
    "for idx, filename in enumerate(sample_files):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    filepath = os.path.join(first_class_dir, filename)\n",
    "    y, sr = librosa.load(filepath, sr=16000)\n",
    "    \n",
    "    ax.plot(np.linspace(0, len(y)/sr, len(y)), y)\n",
    "    ax.set_title(filename, fontsize=10)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Processed Audio Samples - {first_class}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dataset size comparison\n",
    "classes = dataset_info['classes']\n",
    "original_counts = [dataset_info['statistics'][c]['original_files'] for c in classes]\n",
    "processed_counts = [dataset_info['statistics'][c]['processed_files'] for c in classes]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, original_counts, width, label='Original', color='steelblue')\n",
    "ax.bar(x + width/2, processed_counts, width, label='After Processing', color='coral')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Number of Files')\n",
    "ax.set_title('Dataset Size: Before and After Processing', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classes, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProcessed files saved to: ../data/processed/\")\n",
    "print(f\"Dataset info saved to: ../data/processed/dataset_info.json\")\n",
    "print(\"\\nNext Step: Run 03_yamnet_embeddings.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
